"use strict";(self.webpackChunkmcp_ai=self.webpackChunkmcp_ai||[]).push([[3518],{4369:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/openai-ui-sdk-and-mcp-chatgpt","metadata":{"permalink":"/openai-ui-sdk-and-mcp-chatgpt","source":"@site/blog/openai-ui-sdk-and-mcp-chatgpt.mdx","title":"Integrate the OpenAI UI SDK for ChatGPT With HAPI MCP in 60 Seconds","description":"Step-by-step guide to turn any OpenAPI 3.0+ spec into a ChatGPT-ready MCP server using HAPI MCP and the OpenAI UI SDK. Fast deployment, validation, and optimization for Generative and Answer Engine search.","date":"2025-11-24T17:48:55.000Z","tags":[{"inline":false,"label":"MCP","permalink":"/tags/mcp","description":"MCP is an open protocol for connecting LLM apps to external data sources and tools, enabling seamless integration and interoperability.\\n"},{"inline":false,"label":"OpenAI","permalink":"/tags/openai","description":"OpenAI is an AI research and deployment company focused on ensuring that artificial general intelligence benefits all of humanity.\\n"},{"inline":false,"label":"API-First","permalink":"/tags/api-first","description":"API-First is a design approach that prioritizes the development of APIs before building the actual application, ensuring better integration and scalability.\\n"},{"inline":false,"label":"LLM","permalink":"/tags/llm","description":"Large Language Models (LLMs) are advanced AI models trained on vast text data to understand and generate human-like language.\\n"},{"inline":false,"label":"Guide","permalink":"/tags/guide","description":"Curated instructions or documentation designed to help users understand and implement specific use cases around MCP and related technologies.\\n"},{"inline":false,"label":"AI Integration","permalink":"/tags/ai-integration","description":"AI Integration involves incorporating artificial intelligence technologies into existing systems and workflows to enhance functionality and performance.\\n"}],"readingTime":5.03,"hasTruncateMarker":true,"authors":[{"name":"Adrian Escutia","title":"La Rebelion Founder","url":"https://adrian.escutia.me","page":{"permalink":"/authors/adrian"},"socials":{"x":"https://x.com/ades_mx","linkedin":"https://www.linkedin.com/in/adrianescutia/","github":"https://github.com/adrianescutia","newsletter":"https://rebelion.la"},"bio":"Adrian is the founder of La Rebelion, a newsletter about AI, technology, and the future. He is also a software engineer and entrepreneur.","imageURL":"https://github.com/adrianescutia.png","key":"adrian"}],"frontMatter":{"title":"Integrate the OpenAI UI SDK for ChatGPT With HAPI MCP in 60 Seconds","description":"Step-by-step guide to turn any OpenAPI 3.0+ spec into a ChatGPT-ready MCP server using HAPI MCP and the OpenAI UI SDK. Fast deployment, validation, and optimization for Generative and Answer Engine search.","tags":["mcp","openai","api-first","llm","guide","ai-integration"],"authors":["adrian"],"image":"/img/make-mcp-hapi.png"},"unlisted":false,"nextItem":{"title":"Composable Architectures and the Future of AI Integration: Why MCP Is the Missing Link","permalink":"/composable-architectures-and-the-future-of-ai-integration"}},"content":"You want your API to be instantly usable inside AI assistants like ChatGPT, and Web UIs. The OpenAI UI SDK plus the Model Context Protocol (MCP) lets you expose your API as structured tools that large language models can call directly.\\n\\nIn this hands-on guide, you deploy a fully functional MCP server from any OpenAPI 3.0 specification and make it usable by ChatGPT and other MCP-aware clients in under a minute using HAPI MCP. You learn to deploy using cloud and on\u2011premises workflows.\\n\\n\x3c!-- truncate --\x3e\\n\\nJust two months ago, I wrote about how [OpenAI announced the custom MCP](https://rebelion.la/chatgpt-meets-custom-mcps) (Model Context Protocol) interface to allow developers to integrate their APIs directly into ChatGPT, but there was [this caveat](https://rebelion.la/chatgpt-meets-custom-mcps#heading-the-caveat-search-and-fetch): Developers would need to implement two tools, a \\"search\\" tool to find relevant information and a \\"fetch\\" tool to retrieve that information. Today, with the OpenAI UI SDK, that process is greatly simplified.\\n\\n## Why Integrate Your API With the OpenAI UI SDK via MCP?\\n\\n* You expose real-time, authoritative data to ChatGPT and other MCP-aware clients.\\n* You eliminate brittle prompt engineering for retrieval and action execution.\\n* You accelerate feature delivery\u2014turn any supported OpenAPI spec into tool metadata.\\n\\n## Architecture Overview\\n\\nThe deployment flow:\\n\\n1. Provide an OpenAPI 3.0+ JSON or YAML spec.\\n2. HAPI MCP ingests paths, methods, parameters, and schemas.\\n3. Tools are generated with typed argument definitions.\\n4. The MCP server exposes a JSON-RPC style interface consumable by the OpenAI UI SDK or other MCP-clients.\\n5. Clients query available tools, then issue structured calls.\\n\\n### Key Components\\n\\n| Component | Role |\\n| :-- | :-- |\\n| OpenAPI Spec | Source of truth for endpoints, params, schemas |\\n| HAPI MCP CLI / Cloud | Transforms spec \u2192 MCP tool registry |\\n| MCP Server | Serves tool metadata and handles invocations |\\n| Client Runtime | ChatGPT / custom app calling tools |\\n\\n## Prerequisites\\n\\n* An OpenAPI 3.0+ spec URL or local file (JSON or YAML).\\n* A workstation with terminal access, only if you plan to use the HAPI MCP CLI (on-premises deployment).\\n* Basic familiarity with REST API concepts (no MCP experience required).\\n* Optional: A converted OAS 2.0 (Swagger) spec if you start from legacy format.\\n\\n## Quick Answer: What Is the Fastest Way to Get an API Inside ChatGPT?\\n\\nProvide an OpenAPI 3.0 spec to HAPI MCP (cloud or CLI), start the MCP server, then [connect ChatGPT](https://developers.openai.com/apps-sdk/deploy/connect-chatgpt) custom tools configuration at the generated endpoint.\\n\\n## Cloud Deployment (Fastest Path)\\n\\n1. Open the run portal: `https://run.mcp.com.ai/?oas=<YOUR_OPENAPI_SPEC_URL>&apiServer=<OPTIONAL_BASE_URL>`\\n2. Replace `<YOUR_OPENAPI_SPEC_URL>` with a direct spec URL (must be publicly accessible).\\n3. (Optional) Replace `<OPTIONAL_BASE_URL>` with your API server base URL if needed - not all specs include server definitions.\\n4. Wait for provisioning; you receive an MCP server endpoint URL.\\n5. Use that URL in the ChatGPT Apps & Connectors settings.\\n6. Validate tool listing (see Validation section below).\\n\\n### Example URL\\n\\n```console\\nhttps://run.mcp.com.ai/?oas=https://example.com/openapi.json\\n```\\n\\n## On-Premises Deployment With HAPI MCP CLI\\n\\n1. Install the CLI\\n[Download the latest release](https://github.com/la-rebelion/hapimcp/releases) and install the HAPI MCP CLI executable, depending on your OS. \\n\\nOr install via Bun/npm:\\n```bash\\nbun install -g @la-rebelion/hapimcp\\n```\\n2. Verify installation:\\n```bash\\nhapi --version\\n```\\n1. Start a local MCP server (using a spec alias - located in `HAPI_HOME/specs`):\\n```bash\\nhapi serve petstore --headless\\n```\\n1. Or specify a direct spec URL:\\n```bash\\nhapi serve my-api --openapi=\\"https://example.com/openapi.json\\" --headless\\n```\\n1. Capture the printed MCP server endpoint (e.g., `http://localhost:3000/mcp`).\\n2. Add that URL to ChatGPT Apps & Connectors settings.\\n\\n## Deploy to Cloud via CLI\\n\\n1. Run:\\n```bash\\nhapi deploy strava --var HAPI_OPENAPI:\\"https://docs.mcp.com.ai/apis/openapi/strava.json\\"\\n```\\n1. Use `--dry-run` to inspect generated YAML or JSON before committing:\\n```bash\\nhapi deploy strava --var HAPI_OPENAPI:\\"https://docs.mcp.com.ai/apis/openapi/strava.json\\" --dry-run\\n```\\n1. Store returned endpoint URL for ChatGPT integration.\\n\\n## Converting Swagger (OAS 2.0) to OpenAPI 3.0+\\n\\nDon\'t have an OpenAPI 3.0+ spec? Convert from Swagger (OAS 2.0) using these steps:\\n\\n1. Use the official online converter:\\n```bash\\ncurl -X POST -H \\"Content-Type: application/json\\" \\\\\\n  -d @swagger.json https://converter.swagger.io/api/convert \\\\\\n  -o openapi3.json\\n```\\n1. Confirm version:\\n```bash\\ngrep \'\\"openapi\\"\' openapi3.json\\n```\\n1. Fix any `schema` vs `definitions` mismatches manually if needed.\\n1. Retry deployment with the new file.\\n\\n## Validating Your MCP Server\\n\\n1. List tools:\\n```bash\\ncurl -s https://your-mcp-endpoint.example/mcp/tools | jq\\n```\\n1. Call a tool (example POST body):\\n```bash\\ncurl -X POST https://your-mcp-endpoint.example/mcp/call \\\\\\n  -H \'Content-Type: application/json\' \\\\\\n  -d \'{\\n    \\"tool\\":\\"getPetById\\",\\n    \\"params\\":{\\"petId\\": 123}\\n  }\'\\n```\\n1. Check response latency; target < 1s for best conversational UX.\\n1. Log errors; ensure input validation messages are concise and actionable.\\n1. Confirm schema alignment with expected OpenAPI parameter types.\\n\\n## Frequently Asked Questions\\n\\n### How fast can you expose an API to ChatGPT?\\nYou can expose a spec-driven API in under 60 seconds using the cloud portal or a single CLI command.\\n\\n### Do you need to write glue code?\\nYou do not need custom tool wrappers; HAPI MCP converts spec operations automatically.\\n\\n### Can you use private specs?\\nYes. Host the spec behind authenticated access or load from a local file via CLI.\\n\\n### What formats are supported?\\nOpenAPI 3.0+ JSON or YAML. Convert OAS 2.0 (Swagger) before deployment.\\n\\n### Is this limited to ChatGPT?\\nNo. Any MCP-aware client (custom apps, open-source chat UIs) can consume the server.\\n\\n## Conclusion\\n\\nYou now have a repeatable path to transform any OpenAPI 3.0+ specification into a production-ready MCP server consumable by the OpenAI UI SDK and AI assistants. By following validation, conversion, and optimization steps, you improve tool discoverability. Next, refine schema descriptions and add automated integration tests to ensure durable quality as your API evolves.\\n\\nYour APIs are now ready to be seamlessly integrated into ChatGPT and other MCP-aware clients, unlocking new possibilities for AI-driven interactions. Stop wasting time writing MCP Server code manually\u2014leverage HAPI MCP to accelerate your AI integration journey!\\n\\nBe HAPI and Go Rebels! \u270a\ud83c\udffd"},{"id":"/composable-architectures-and-the-future-of-ai-integration","metadata":{"permalink":"/composable-architectures-and-the-future-of-ai-integration","source":"@site/blog/composable-architectures-and-the-future-of-ai-integration.md","title":"Composable Architectures and the Future of AI Integration: Why MCP Is the Missing Link","description":"Instead of rebuilding APIs, learn how composable architectures and MCP Gateways help enterprises move from API-First to AI-First.","date":"2025-11-01T15:05:34.000Z","tags":[{"inline":false,"label":"MCP","permalink":"/tags/mcp","description":"MCP is an open protocol for connecting LLM apps to external data sources and tools, enabling seamless integration and interoperability.\\n"},{"inline":false,"label":"AI Integration","permalink":"/tags/ai-integration","description":"AI Integration involves incorporating artificial intelligence technologies into existing systems and workflows to enhance functionality and performance.\\n"},{"inline":false,"label":"LLM","permalink":"/tags/llm","description":"Large Language Models (LLMs) are advanced AI models trained on vast text data to understand and generate human-like language.\\n"},{"inline":false,"label":"API","permalink":"/tags/api","description":"An API (Application Programming Interface) is a set of rules and protocols that allows different software applications to communicate and interact with each other.\\n"},{"inline":false,"label":"MCP Gateway","permalink":"/tags/mcp-gateway","description":"An MCP Gateway acts as a bridge between existing APIs and the Model Context Protocol, enabling seamless integration of legacy systems with AI applications.\\n"}],"readingTime":2.5,"hasTruncateMarker":true,"authors":[{"name":"Adrian Escutia","title":"La Rebelion Founder","url":"https://adrian.escutia.me","page":{"permalink":"/authors/adrian"},"socials":{"x":"https://x.com/ades_mx","linkedin":"https://www.linkedin.com/in/adrianescutia/","github":"https://github.com/adrianescutia","newsletter":"https://rebelion.la"},"bio":"Adrian is the founder of La Rebelion, a newsletter about AI, technology, and the future. He is also a software engineer and entrepreneur.","imageURL":"https://github.com/adrianescutia.png","key":"adrian"}],"frontMatter":{"title":"Composable Architectures and the Future of AI Integration: Why MCP Is the Missing Link","description":"Instead of rebuilding APIs, learn how composable architectures and MCP Gateways help enterprises move from API-First to AI-First.","tags":["mcp","ai-integration","llm","api","mcp-gateway"],"keywords":["MCP","Model Context Protocol","Composable AI architecture","AI Integration","LLMs","APIs","MCP Gateway","future of AI system integration","API-First vs AI-First transformation"],"image":"https://images.unsplash.com/photo-1492355040260-cd982083603e?ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&q=80&w=1170","authors":["adrian"]},"unlisted":false,"prevItem":{"title":"Integrate the OpenAI UI SDK for ChatGPT With HAPI MCP in 60 Seconds","permalink":"/openai-ui-sdk-and-mcp-chatgpt"},"nextItem":{"title":"Using OpenAI Response API for MCP Integration","permalink":"/use-openai-api-for-smooth-mcp-integration"}},"content":"For years, **OpenAPI (OAS)** has been the cornerstone of how we describe, document, and integrate APIs. It standardized how systems talk to each other \u2014 and in many ways, it made the modern internet possible.\\n\\nSo when the **Model Context Protocol (MCP)** appeared, some developers were skeptical.\\n\\"Another spec?\\" they asked. \\"Why do we need this when we already have OAS?\\"\\n\\n\x3c!-- truncate --\x3e\\n\\nThat skepticism was fair \u2014 until people started to realize **what MCP actually unlocks**:\\n\ud83d\udc49 **Tool discovery for Large Language Models (LLMs)**.\\n\\nThis is the real spark \u2014 the value proposition that gave MCP its traction. It\'s not just another API spec; it\'s the bridge between traditional APIs and the AI agents that can use them.\\n\\n---\\n\\n### Why Re-Invent When You Can Re-Compose?\\n\\nMany enterprises today already have a vast API ecosystem \u2014 especially in industries like **telecommunications**, **finance**, or **healthcare**.\\nHundreds of services, standardized and custom, all tested, audited, and certified.\\n\\nSo when these organizations hear about \\"MCP servers,\\" it\'s natural to worry that they\'ll need to rebuild everything. But here\'s the truth:\\n\\n> **You don\'t need to re-implement your APIs as MCP servers. You just need to make them discoverable.**\\n\\nThat\'s where the **MCP Gateway** comes in.\\n\\n---\\n\\n### The Role of the MCP Gateway\\n\\nThink of an MCP Gateway as a **translator and orchestrator** between your existing APIs and AI-driven agents.\\n\\n* It receives MCP requests from LLMs.\\n* It routes them to your existing APIs.\\n* It transforms the responses back into MCP format.\\n\\nNo reinvention. No duplicate logic. Just a smarter bridge between systems.\\n\\nAnd because it sits at the intersection of APIs and AI, the MCP Gateway can handle the enterprise-grade essentials too \u2014\\n\u2705 Authentication & authorization\\n\u2705 Rate limiting\\n\u2705 Logging & observability\\n\u2705 Policy enforcement & compliance\\n\\nIt\'s the perfect layer to keep everything **secure, compliant, and manageable** while opening the door to AI-powered integration.\\n\\n---\\n\\n### Composable Architectures: The Future of Integration\\n\\nWe\'ve moved from monoliths to microservices, and now from **API-First to AI-First**.\\nThe next evolution is **composability** \u2014 the ability to assemble, re-use, and extend existing systems dynamically.\\n\\nMCP Gateways embody this principle.\\nThey empower teams to:\\n\\n* Reuse what already works.\\n* Expose APIs as AI-ready tools.\\n* Integrate with LLMs and agents seamlessly.\\n* Accelerate innovation instead of rewriting history.\\n\\nEnterprises can finally focus on **intent matching** \u2014 connecting what an AI agent *wants to do* with the right existing service, without reinventing the wheel.\\n\\n---\\n\\n### The Mindset Shift\\n\\nAdopting MCP isn\'t just a technical change \u2014 it\'s a cultural one.\\nIt\'s about moving from building APIs *for humans to consume* to building systems that *AI can understand and compose*.\\n\\nIt\'s about trust \u2014 trusting that your existing architecture already has value, and that the future doesn\'t require you to start over, but to **connect smarter**.\\n\\n---\\n\\n### Final Thought\\n\\n**Composable, AI-ready architectures are the future of integration.**\\nMCP Gateways are the missing link \u2014 enabling enterprises to evolve from *API-First* to *AI-First* without disruption.\\n\\nBecause innovation shouldn\'t mean rebuilding everything.\\nIt should mean **unlocking what you already have** \u2014 and letting intelligence flow through it."},{"id":"/use-openai-api-for-smooth-mcp-integration","metadata":{"permalink":"/use-openai-api-for-smooth-mcp-integration","source":"@site/blog/use-openai-api-for-smooth-mcp-integration.mdx","title":"Using OpenAI Response API for MCP Integration","description":"A step-by-step guide to integrating OpenAI\'s Response API with Model Context Protocol (MCP) using HAPI Server.","date":"2025-11-01T15:05:34.000Z","tags":[{"inline":false,"label":"MCP","permalink":"/tags/mcp","description":"MCP is an open protocol for connecting LLM apps to external data sources and tools, enabling seamless integration and interoperability.\\n"},{"inline":false,"label":"OpenAI","permalink":"/tags/openai","description":"OpenAI is an AI research and deployment company focused on ensuring that artificial general intelligence benefits all of humanity.\\n"},{"inline":false,"label":"API-First","permalink":"/tags/api-first","description":"API-First is a design approach that prioritizes the development of APIs before building the actual application, ensuring better integration and scalability.\\n"},{"inline":false,"label":"LLM","permalink":"/tags/llm","description":"Large Language Models (LLMs) are advanced AI models trained on vast text data to understand and generate human-like language.\\n"},{"inline":false,"label":"Guide","permalink":"/tags/guide","description":"Curated instructions or documentation designed to help users understand and implement specific use cases around MCP and related technologies.\\n"},{"inline":false,"label":"AI Integration","permalink":"/tags/ai-integration","description":"AI Integration involves incorporating artificial intelligence technologies into existing systems and workflows to enhance functionality and performance.\\n"}],"readingTime":4.12,"hasTruncateMarker":true,"authors":[{"name":"Adrian Escutia","title":"La Rebelion Founder","url":"https://adrian.escutia.me","page":{"permalink":"/authors/adrian"},"socials":{"x":"https://x.com/ades_mx","linkedin":"https://www.linkedin.com/in/adrianescutia/","github":"https://github.com/adrianescutia","newsletter":"https://rebelion.la"},"bio":"Adrian is the founder of La Rebelion, a newsletter about AI, technology, and the future. He is also a software engineer and entrepreneur.","imageURL":"https://github.com/adrianescutia.png","key":"adrian"}],"frontMatter":{"title":"Using OpenAI Response API for MCP Integration","description":"A step-by-step guide to integrating OpenAI\'s Response API with Model Context Protocol (MCP) using HAPI Server.","tags":["mcp","openai","api-first","llm","guide","ai-integration"],"keywords":["MCP","Model Context Protocol","OpenAI Response API","HAPI Server","API Integration","LLMs","Taco Ordering System","Postman AI Agent"],"image":"https://images.unsplash.com/photo-1676299081847-824916de030a?ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&q=80&w=1170","authors":["adrian"]},"unlisted":false,"prevItem":{"title":"Composable Architectures and the Future of AI Integration: Why MCP Is the Missing Link","permalink":"/composable-architectures-and-the-future-of-ai-integration"},"nextItem":{"title":"MCP Threats and Misleadings - Prompt Injection","permalink":"/mcp-threats-prompt-injection"}},"content":"import ReactPlayer from \'react-player\'\\n\\nThis guide demonstrates how to use the **OpenAI Response API** to integrate with the **Model Context Protocol (MCP)** for seamless, AI-driven API interactions.\\nYou will learn how to generate a REST API specification with Postman\'s AI Agent, deploy it as an **MCP server using HAPI Server**, and connect it through OpenAI\'s **Response API** for testing.\\n\\n\x3c!-- truncate --\x3e\\n\\nThe goal of this demo is to show how OpenAI\'s new **Tools and Connectors for MCP** feature simplifies integration between large language models and your APIs, making it possible to interact with services such as a \\"Taco Ordering System\\" directly from an LLM client like ChatGPT or a Bun-based application.\\n\\nBy the end of this guide, you will have a working MCP server derived from a Swagger specification, exposed via ngrok, and connected to the OpenAI Response API for end-to-end interaction testing.\\n\\n\\n---\\n\\n## Prerequisites\\n\\nBefore you begin:\\n\\n* Install the following tools:\\n\\n  * [Bun](https://bun.sh/) \u2013 A fast JavaScript runtime for modern apps.\\n    * Alternatively, you can use Node.js if preferred.\\n  * [ngrok](https://ngrok.com/) \u2013 To expose your local server to the internet.\\n  * [HAPI Server](https://hapi.mcp.com.ai) \u2013 To host your MCP server.\\n  * [OpenAI SDK](https://www.npmjs.com/package/openai) \u2013 For client testing.\\n  * Obtain or install **Postman Desktop Agent** to generate APIs using AI.\\n    * Alternatively, you can ask **ChatGPT** to generate a Swagger specification (v3).\\n\\n:::info\\nThe example below uses a local environment and ngrok for simplicity, but you can deploy your server on any cloud instance or containerized environment.\\n:::\\n\\n---\\n\\n## Step 1: Generate an API Using Postman Agent\\n\\nYou can use Postman\'s AI Agent to quickly generate an API for any use case.\\nIn this example, you will create a simple Taco Ordering System.\\n\\n1. Open Postman\'s AI Agent and prompt:\\n\\n   ```\\n   Generate an API for a Taco ordering system with endpoints to view tacos and place an order.\\n   ```\\n2. The agent returns a Swagger (OpenAPI) specification for your taco API.\\n3. Review the generated specification to confirm it includes paths for:\\n\\n   * `GET /tacos`\\n   * `POST /order`\\n\\n:::info\\nSave the file in your `HAPI_HOME/specs` directory as `tacos.yaml` for use in the next step. (JSON format is also acceptable.)\\n:::\\n\\n---\\n\\n## Step 2: Start an MCP Server Using HAPI Server\\n\\nUse the [HAPI Server](https://hapi.mcp.com.ai) to host your Swagger API as an MCP server. This allows LLM-based clients such as ChatGPT to call your API directly.\\n\\n1. Connect to your instance and run the following command to start the MCP server:\\n\\n   ```bash\\n   bunx @la-rebelion/hapimcp serve tacos --headless --chatgpt\\n   ```\\n   \\n   Alternatively, you can use the native CLI (download from [GitHub releases](https://github.com/la-rebelion/hapimcp/releases)) and run:\\n\\n   ```bash\\n   hapi serve tacos --headless --chatgpt\\n   ```\\n2. Once running, expose the local server to the internet using ngrok:\\n\\n   ```bash\\n   ngrok http 8000\\n   ```\\n3. Copy the public URL generated by ngrok (e.g., `https://abcd1234.ngrok.io`).\\n\\n---\\n\\n## Step 3: Connect ChatGPT or Another Client to the MCP Server\\n\\n1. Open ChatGPT\'s settings and add a new connector with the ngrok URL you copied earlier.\\n2. Enable the connector.\\n   If the feature is not visible, note that **MCP connectors** in ChatGPT may still be in **beta**.\\n3. Alternatively, you can connect directly from an MCP-compatible client using the OpenAI SDK.\\n\\n---\\n\\n## Step 4: Test the MCP Server With OpenAI SDK\\n\\nYou can test the endpoint by sending a prompt that interacts with your Taco API.\\n\\n1. Initialize a new Bun project:\\n\\n   ```bash\\n   mkdir taco-client && cd taco-client\\n   bun init\\n   ```\\n2. Install dependencies:\\n\\n   ```bash\\n   bun install openai\\n   ```\\n3. Create an `index.ts` file and configure your endpoint:\\n\\n   ```typescript\\n   import OpenAI from \\"openai\\";\\n\\n   const client = new OpenAI({\\n     apiKey: process.env.OPENAI_API_KEY,\\n     baseURL: \\"https://abcd1234.ngrok.io/v1\\"\\n   });\\n\\n   const response = await client.chat.completions.create({\\n     model: \\"gpt-4.1\\",\\n     messages: [{ role: \\"user\\", content: \\"What tacos do you have in the menu?\\" }]\\n   });\\n\\n   console.log(response.choices[0].message);\\n   ```\\n4. Run the client:\\n\\n   ```bash\\n   bun run index.ts\\n   ```\\n\\nYou should see a response such as:\\n\\n```\\nWe have \\"carne asada\\" and \\"al pastor\\" tacos available.\\n```\\n\\n---\\n\\n## Step 5: Place an Order Through the API\\n\\nNow that your MCP server is connected, you can test the ordering functionality.\\n\\nRun:\\n\\n```bash\\nbun run index.ts\\n```\\n\\nWith the following request:\\n\\n```typescript\\nmessages: [\\n  { role: \\"user\\", content: \\"Place an order for 2 tacos al pastor and 1 carne asada.\\" }\\n]\\n```\\n\\nThe client confirms your order:\\n\\n```\\nOrder placed successfully!  \\nYou ordered 2 tacos al pastor and 1 carne asada.\\n```\\n\\n## Demo Video\\n\\n<ReactPlayer\\n  src=\'https://youtu.be/C6tz7K2og6I\'\\n  style={{ width: \'90%\', height: \'auto\', aspectRatio: \'4/3\' }}\\n  controls\\n/>\\n\\n---\\n\\n## Conclusion\\n\\nYou successfully:\\n\\n* Generated a Swagger API with Postman\'s AI Agent.\\n* Deployed it as an MCP server using HAPI Server.\\n* Exposed the service securely with ngrok.\\n* Connected and interacted with it using ChatGPT and Bun.\\n\\nThis setup allows you to prototype **API-first AI integrations** quickly. You can expand this demo to connect more endpoints or integrate it into a cluster for scalability.\\n\\n:::info\\nTo continue exploring, deploy your HAPI MCP server on **cloud instances** or integrate it into your existing CI/CD pipeline for real-world workloads.\\n:::\\n\\nBe HAPI, and go Rebels! \u270a\ud83c\udffd"},{"id":"/mcp-threats-prompt-injection","metadata":{"permalink":"/mcp-threats-prompt-injection","source":"@site/blog/mcp-threats-prompt-injection.md","title":"MCP Threats and Misleadings - Prompt Injection","description":"An overview of threats and misleadings related to prompt injection in the context of MCP (Model Context Protocol).","date":"2025-10-08T11:36:31.000Z","tags":[{"inline":false,"label":"MCP","permalink":"/tags/mcp","description":"MCP is an open protocol for connecting LLM apps to external data sources and tools, enabling seamless integration and interoperability.\\n"},{"inline":false,"label":"Security","permalink":"/tags/security","description":"Security involves protecting systems, networks, and data from digital attacks, unauthorized access, and damage\\n"}],"readingTime":3.15,"hasTruncateMarker":true,"authors":[{"name":"Adrian Escutia","title":"La Rebelion Founder","url":"https://adrian.escutia.me","page":{"permalink":"/authors/adrian"},"socials":{"x":"https://x.com/ades_mx","linkedin":"https://www.linkedin.com/in/adrianescutia/","github":"https://github.com/adrianescutia","newsletter":"https://rebelion.la"},"bio":"Adrian is the founder of La Rebelion, a newsletter about AI, technology, and the future. He is also a software engineer and entrepreneur.","imageURL":"https://github.com/adrianescutia.png","key":"adrian"}],"frontMatter":{"title":"MCP Threats and Misleadings - Prompt Injection","description":"An overview of threats and misleadings related to prompt injection in the context of MCP (Model Context Protocol).","authors":["adrian"],"tags":["mcp","security"],"image":"https://cdn.gamma.app/z3n406kubdfbysb/generated-images/trv8ZioPQIQ84iK46Bd8B.png"},"unlisted":false,"prevItem":{"title":"Using OpenAI Response API for MCP Integration","permalink":"/use-openai-api-for-smooth-mcp-integration"},"nextItem":{"title":"Swagger/OAS v4 Is out","permalink":"/oas-v4-is-out"}},"content":"Prompt injection is a critical security risk for any system using large language models (LLMs), including those built with Model Context Protocol (MCP). You must understand how prompt injection works, why MCP cannot prevent it, and what steps you should take to protect your users and applications (MCP Clients).\\n\\n\x3c!-- truncate --\x3e\\n\\n## Introduction\\n\\nMCP enables users and clients to discover and pull prompts from MCP Servers. This flexibility means you, as a client developer, are responsible for validating and sanitizing all user inputs before they reach the LLM. MCP Clients act as intermediaries between MCP Servers, end users, and LLMs. If you do not implement proper validation, malicious prompts can reach the LLM and cause harmful or unintended outputs.\\n\\n:::warning\\nPrompt injection is recognized as a top risk by OpenAI ([OpenAI Prompt Injection Guide](https://platform.openai.com/docs/guides/prompt-injection)), Anthropic ([Anthropic Prompt Injection FAQ](https://www.anthropic.com/index/prompt-injection)), and OWASP ([OWASP AI Security Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/)).\\n:::\\n\\n## How Prompt Injection Works in MCP\\n\\n```mermaid\\nflowchart RL\\n  A[Client Developer] --\x3e|Builds| E[MCP Client]\\n  E --\x3e|Connects to| B[MCP Server]\\n  B --\x3e|Serves Prompts| E\\n  E --\x3e|Delivers Prompts| C[End User]\\n  C --\x3e|Inputs/Queries| E\\n  E --\x3e|Forwards Inputs| D[LLM]\\n  D --\x3e|Generates Responses| E\\n  E --\x3e|Returns Responses| C\\n  B --\x3e|Provides Prompts| E\\n  subgraph Perspectives\\n    E\\n    C\\n    D\\n  end\\n```\\n\\n**Key Roles:**\\n- **Client Developer:** You build MCP Clients and must implement security and validation.\\n- **MCP Client:** Your app acts as an intermediary, handling prompt delivery and user input forwarding.\\n- **End User:** Users interact with LLMs via MCP Clients and may provide inputs that could be exploited.\\n- **LLM:** Processes prompts and user inputs, vulnerable to prompt injection if upstream validation is insufficient.\\n\\n## Threats\\n\\nPrompt injection can lead to several serious risks:\\n\\n- **Malicious Prompts:** Attackers craft prompts to manipulate the LLM into generating harmful or unintended outputs ([OpenAI](https://platform.openai.com/docs/guides/prompt-injection)).\\n- **User Input Manipulation:** End users may input data designed to exploit vulnerabilities in the LLM\'s response generation ([Anthropic](https://www.anthropic.com/index/prompt-injection)).\\n- **Data Leakage:** Sensitive information may be exposed through manipulated prompts or responses ([NIST AI RMF](https://airmf.nist.gov/)).\\n- **Reputation Damage:** Misleading outputs generated by LLMs due to prompt injection can harm your application\'s reputation.\\n\\n## Common Misleadings\\n\\nAvoid these misconceptions:\\n\\n- **False Sense of Security:** MCP does not provide inherent protection against prompt injection. You must implement your own safeguards.\\n- **Overreliance on LLMs:** LLMs do not automatically handle all types of inputs safely. Validation is essential.\\n- **Misunderstanding Roles:** Security measures must be implemented in the MCP Client, not just the server or LLM.\\n- **Assumption of Trustworthiness:** Do not trust all prompts from third-party MCP Servers. Use official or self-hosted servers when possible ([OWASP](https://owasp.org/www-project-top-10-for-large-language-model-applications/)).\\n- **Neglecting Client Responsibility:** Input validation is your responsibility as the MCP Client developer.\\n\\n## Mitigation Strategies\\n\\nFollow these best practices to reduce prompt injection risks:\\n\\n1. **Input Validation:** Check all user inputs for malicious content before forwarding to the LLM ([OWASP](https://owasp.org/www-project-top-10-for-large-language-model-applications/)).\\n2. **Sanitization:** Remove or neutralize potentially harmful elements in user inputs.\\n3. **User Education:** Inform users about prompt injection risks and safe input practices.\\n4. **Regular Audits:** Audit your MCP Client regularly to identify and fix vulnerabilities.\\n5. **Monitoring and Logging:** Track interactions and flag suspicious activities for review.\\n\\n## References\\n\\n- [OpenAI: Prompt Injection Guide](https://platform.openai.com/docs/guides/prompt-injection)\\n- [Anthropic: Prompt Injection FAQ](https://www.anthropic.com/index/prompt-injection)\\n- [OWASP: Top 10 for LLM Applications](https://owasp.org/www-project-top-10-for-large-language-model-applications/)\\n- [NIST: AI Risk Management Framework](https://airmf.nist.gov/)\\n\\nBy understanding and addressing these threats, you can secure your MCP Client against prompt injection vulnerabilities and protect your users and reputation."},{"id":"/oas-v4-is-out","metadata":{"permalink":"/oas-v4-is-out","source":"@site/blog/oas-v4-is-out.mdx","title":"Swagger/OAS v4 Is out","description":"Exploring the potential of OpenAPI Specification (OAS) v4 for AI tool integration and how it can complement Model Context Protocol (MCP).","date":"2025-10-08T11:36:31.000Z","tags":[{"inline":false,"label":"OAS","permalink":"/tags/oas","description":"OpenAPI Specification\u2014industry standard for describing RESTful APIs and enabling interoperability."},{"inline":false,"label":"MCP","permalink":"/tags/mcp","description":"MCP is an open protocol for connecting LLM apps to external data sources and tools, enabling seamless integration and interoperability.\\n"},{"inline":false,"label":"API","permalink":"/tags/api","description":"An API (Application Programming Interface) is a set of rules and protocols that allows different software applications to communicate and interact with each other.\\n"},{"inline":false,"label":"AI","permalink":"/tags/ai","description":"Artificial Intelligence (AI) is the simulation of human intelligence in machines that are programmed to think and learn like humans.\\n"}],"readingTime":9.81,"hasTruncateMarker":true,"authors":[{"name":"Adrian Escutia","title":"La Rebelion Founder","url":"https://adrian.escutia.me","page":{"permalink":"/authors/adrian"},"socials":{"x":"https://x.com/ades_mx","linkedin":"https://www.linkedin.com/in/adrianescutia/","github":"https://github.com/adrianescutia","newsletter":"https://rebelion.la"},"bio":"Adrian is the founder of La Rebelion, a newsletter about AI, technology, and the future. He is also a software engineer and entrepreneur.","imageURL":"https://github.com/adrianescutia.png","key":"adrian"}],"frontMatter":{"title":"Swagger/OAS v4 Is out","description":"Exploring the potential of OpenAPI Specification (OAS) v4 for AI tool integration and how it can complement Model Context Protocol (MCP).","authors":["adrian"],"tags":["oas","mcp","api","ai"],"image":"/img/ai-friend-chatting.png"},"unlisted":false,"prevItem":{"title":"MCP Threats and Misleadings - Prompt Injection","permalink":"/mcp-threats-prompt-injection"}},"content":"import ReactPlayer from \'react-player\'\\n\\n\ud83d\udea8 \\"OpenAPI Specification (OAS) v4 is out\\" - *That I wish*, this is the kind of headline I would expect to see soon, because OAS can easily be extended to enable RESTful APIs work seamlessly with AI. \\n\\nBy the end of this article, you\'ll know how to let any LLM call your REST tools automatically using OAS.\\n\\n\x3c!-- truncate --\x3e\\n\\n**What Does It Mean for AI Tool Integration?** Let\'s explore how OAS v4 could fit into the AI landscape, complementing protocols like Model Context Protocol (MCP). \\n\\n:::note  \\nThis is a speculative article about what I would like to see in OAS v4, based on my experience with Model Context Protocol (MCP) and AI tool integration. OAS v4 is not yet released, and this article is not endorsed by the OpenAPI Initiative or any other organization.  \\n:::\\n\\nOver the past decade, OAS has become the standard for describing RESTful APIs and is widely adopted across various industries. Now, it\'s time to expand it to cover AI use cases, addressing the evolving needs of developers.\\n\\nModel Context Protocol (MCP) has recently gained a lot of attention. It\'s great for local integrations, allows you to adjust the MCP server for backend connections, and with the latest updates, it can now handle remote calls, moving closer to RESTful APIs. \\n\\nSince the beginning [MCP was designed for local integrations mainly](https://rebelion.la/model-context-protocol-mcp-is-it-a-protocol-or-a-contract#heading-the-lsp-connection-understanding-mcps-roots). The breakthrough of MCP lies in its **ability to let the LLM discover the tools available**. This tool discovery feature marks MCP\'s unique value proposition, transforming the way we approach AI integrations. From there, everything falls into place similar to RESTful APIs: the client acts as a lightweight orchestration layer, the server handles the backend tasks, and the LLM plays the role of the brain, deciding which tools to use and how to use them.\\n\\n## How does MCP work? For mere mortals\\n\\nI have noticed that many people struggle to understand how MCP works, so let me try to explain it in a simple way.\\n\\nLet\'s imagine a conversation between the client (you), the LLM (a friend), and the server (weather guy):\\n\\nSetting the scene: you have a person (the MCP Server) who speaks a language you don\'t understand, such as French, and you have a friend who speaks both languages (The LLM), French and English. You\'d like to know the weather in Paris, so you can ask your friend for help. Your friend asks the person in French, gets the answer, and translates it back to you in English. In this scenario, your friend represents the LLM supporting tool calling natively. \\n\\nNow, imagine you can write in French but don\'t speak it fluently. You\'d like to know the weather in Paris, but this time you need someone to guide you on what to ask in French. You write the message asking for the weather. Once you receive the answer, you can translate it without help. This scenario reflects the role of an LLM that doesn\'t support tool calling natively, and the client has to call the tool directly.\\n\\nComplex, right? Let\'s see it in a sequence diagram:\\n\\n```mermaid\\nsequenceDiagram\\n  participant U as User\\n  participant C as Client\\n  participant L as LLM\\n  participant S as Server\\n  U->>C: User input<br/>(e.g., \\"What tacos do you have in the menu?\\")\\n  C->>S: Fetch API spec (tools/list)\\n  S--\x3e>C: API spec (tools [list])\\n  alt LLM supports tool calling\\n    C->>L: API spec (tools [list])\\n    C->>L: User input & context<br/>(user preference, etc)\\n    L--\x3e>C: Tool selection<br/>(assistant suggests a function call)\\n    C->>S: API call\\n  else LLM does not support tool calling\\n    C->>L: User input, tools [list] & context<br/>(user preference, etc)\\n    L--\x3e>C: Tool selection\\n    C->>S: API call<br/>(Client decides protocol, auth, etc)\\n  end\\n  S--\x3e>C: API response\\n  C--\x3e>L: API response\\n  L--\x3e>C: Result to user\\n  C--\x3e>U: Final response\\n```\\n\\nThe LLM receives a prompt with a description of the [available tools](https://modelcontextprotocol.io/specification/2025-06-18/server/tools#listing-tools). This tool list can be [mapped from the OpenAPI spec](https://rebelion.la/you-dont-need-to-implement-mcp-servers-a-contract-first-approach-to-ai-tool-integration?showSharer=true#heading-example-conversion-openapi-mcp). The LLM then chooses which tool to use and how to use it. The client simply passes the user input to the LLM, which decides what to do next. Some LLMs, like GPT-4-turbo and Llama2, support tool calling natively. Others, like Claude, do not, so the client calls the tool directly, based on the LLM\'s instruction.\\n\\nAs you can see, either the LLM supports tool calling natively or not, the flow is similar. The only difference is that in the latter case, the client does the call to the server, and forwards the response to the LLM to get the final result for the end-user - based on the LLM\'s instruction.\\n\\n## OAS v4 - The missing piece\\n\\nThe missing piece in OAS compared to MCP is that OAS does not have a discovery mechanism for the LLM to know which tools are available. This is where OAS v4 could shine, by adding a way for the LLM to discover the API spec and the available tools.\\n\\nImagine if OAS v4 had a way to describe the tools available in a way that the LLM could understand, and then the LLM could decide which tool to use, and how to use it. This would make it possible for any LLM to work seamlessly with RESTful APIs, without the need for a separate protocol like MCP, or even better, MCP could be used as a contract-first approach to AI tool integration, where the OAS v4 spec is the contract that defines the tools available, and the MCP server implements the backend logic. That\'s precisely what I have been advocating for a while now with my [contract-first approach to AI tool integration](https://rebelion.la/you-dont-need-to-implement-mcp-servers-a-contract-first-approach-to-ai-tool-integration?showSharer=true) and the *Headless API* (HAPI) initiative for MCP.\\n\\nNow that I have explained how MCP works, and how OAS v4 could be the missing piece, I hope you can see the potential of OAS v4 for AI tool integration. But, don\'t stop here, next is what I would like to see in OAS v4.\\n\\n## What You Should Expect in OAS v4\\n\\nOAS v4 is not just an incremental update\u2014it\'s an opportunity to rethink how APIs and AI tools work together. Here is what you should expect and advocate for in the next version, based on best practices and the needs of modern AI integrations.\\n\\n## Modular, Multi-File API Specifications\\n\\nYou need modular specs for real-world APIs. OAS v4 should support multi-file specifications natively, allowing you to break down large APIs into smaller, reusable modules (for example, pets, users, orders). This approach enhances maintainability and collaboration across teams. Instead of a single massive file, you organize your API into logical domains:\\n\\n- **API Root:** Main file with info, servers, and tags.\\n- **Security:** Dedicated files for OAuth2 and other schemes.\\n- **Paths by Domain:** Split endpoints into logical groups (e.g. `pet.api.yaml`, `user.api.yaml`).\\n- **Components/Models:** Reusable schemas separated by domain.\\n\\nThis modular structure keeps specs manageable (500\u20131000 lines each) and enables code generators and clients to see one unified spec. Tools like [Redocly CLI](https://redocly.com/docs/cli/file-management#one-large-file-to-many-small-ones) already offer splitting, but OAS v4 should standardize it. Modular specs also help AI tools (MCP servers) consume only the relevant modules, **improving performance and reducing cognitive load for LLMs** that may have context length limitations and make the LLMs\' job easier when deciding which tools to use.\\n\\n:::info\\nThe current `$ref` approach only allows referencing individual components, not entire files. Modular specs let you load what you need, when you need it.\\n:::\\n\\n**Benefits:**\\n- Parallel development and reviews\\n- Domain-specific clients (AI agents load only relevant modules)\\n- Easier maintenance and updates\\n- Improved collaboration\\n- Better tooling support\\n- Granular security schemes\\n\\n## Well-Known Tools Manifest for AI Discovery\\n\\nAI clients must discover available tools easily. OAS v4 should adopt a well-known URI convention (such as `/.well-known/mcp/tools-manifest.json`) so MCP clients and AI agents can auto-discover your API\'s tools. This follows patterns from [OpenID Connect Discovery](https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderConfig) and [OAuth 2.0 Security Scheme](https://spec.openapis.org/oas/v3.2.0.html#security-scheme-object). Give agents a predictable URL to learn your API\u2014this small change pays off big in discoverability.\\n\\n## AI-First Annotations and Metadata\\n\\nOAS v4 should introduce AI-first annotations to help clients and LLMs use APIs effectively. Add metadata extensions (such as `x-llm-hint`, `x-llm-example`) to endpoints, parameters, and responses. Use the [`kind`](https://spec.openapis.org/registry/tag-kind/) property to categorize tags for AI relevance. For example:\\n\\n```yaml\\ntags:\\n  - name: contacts\\n    summary: Manage contacts\\n    description: Endpoints for creating, updating, and searching contacts\\n    kind: ai-tool\\n\\n  - name: internal\\n    summary: Internal admin endpoints\\n    description: Endpoints not intended for AI or public use\\n    kind: internal\\n```\\n\\nWith this approach, AI agents and MCP servers filter and discover only the endpoints relevant for tool calling, ignoring internal or non-AI endpoints. You can further extend this pattern with custom extensions:\\n\\n```yaml\\npaths:\\n  /contacts/search:\\n    get:\\n      tags: [contacts]\\n      x-llm-hint: \\"Use this endpoint to search for contacts by name or email.\\"\\n      x-llm-example: \\"Find all contacts named Alice.\\"\\n```\\n\\nThese annotations guide LLMs on how to use specific endpoints, making your API more AI-friendly and discoverable.\\n\\n## References\\n\\n- [OpenAPI Specification](https://spec.openapis.org/oas/latest.html)\\n- [Redocly CLI: File Management](https://redocly.com/docs/cli/file-management#one-large-file-to-many-small-ones)\\n- [OpenID Connect Discovery](https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderConfig)\\n- [OWASP AI Security Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/)\\n\\nBy advocating for these features, you help shape OAS v4 into a standard that supports scalable, AI-ready APIs for the next generation of applications.\\n\\n## Example: Conversion OpenAPI \u2192 MCP\\n\\nHere is a simple example of how an OpenAPI spec can be converted to an MCP tools manifest.\\n\\nUsing the [Petstore example](https://petstore3.swagger.io/api/v3/openapi.json) from Swagger, we can extract the relevant information to create an MCP tools manifest. Deploying an MCP server with the [`hapi` server](https://docs.mcp.com.ai/components/hapi-server/), we can extend the OpenAPI spec with AI-first annotations and modular structure.\\n\\nIn the demo below, the Petstore API is extended to integrate WorkOS for authentication, and the OpenAPI spec is modularized into separate files for better organization.\\n\\n```yaml\\napiVersion: mcp.com.ai/v1\\nkind: Security\\nmetadata:\\n  name: hapi-security-config\\n  namespace: demo\\n  labels:\\n    app: demo\\n  annotations:\\n    description: HAPI server security configuration\\n    owner: team-hapi\\n    environment: development\\n    version: v1.0.0\\nspec:\\n  cors:\\n    enabled: true\\n    origin:\\n      - http://localhost:8080 # Your MCP Client\\n      - http://localhost:6274 # MCP Inspector\\n    headers:\\n      - Content-Type\\n      - Authorization\\n      - X-Requested-With\\n    methods:\\n      - GET\\n      - POST\\n      - PUT\\n      - DELETE\\n      - OPTIONS\\n  security:\\n    demo_auth:\\n      client_id: client_0\\n      well-known: https://*****.app/.well-known/oauth-authorization-server\\n  securitySchemes:\\n    demo_auth:\\n      type: oauth2\\n      flows:\\n        authorizationCode:\\n          authorizationUrl: https://api.workos.com/sso/authorize?connection=conn_01\\n          tokenUrl: https://*****.app/oauth2/token\\n          scopes:\\n            read: Read\\n            write: Modify\\n            admin: Access to admin operations\\n            read_all: Read private resources\\n```\\n\\n<ReactPlayer\\n  src=\'https://youtu.be/S2_Z0rbnOH8\'\\n  style={{ width: \'90%\', height: \'auto\', aspectRatio: \'4/3\' }}\\n  controls\\n/>\\n\\n## Conclusion\\n\\nOAS v4 has the potential to revolutionize how AI systems interact with RESTful APIs. By incorporating modular specs, AI-first annotations, and well-known discovery endpoints, OAS v4 can make it easier for LLMs to understand and utilize APIs effectively. This would not only benefit developers but also pave the way for more seamless AI integrations across various applications."}]}}')}}]);