"use strict";(self.webpackChunkmcp_ai=self.webpackChunkmcp_ai||[]).push([[3518],{4369:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/openai-meets-mcp","metadata":{"permalink":"/openai-meets-mcp","source":"@site/blog/openai-meets-mcp/index.mdx","title":"OpenAI SDK Meets MCP (Model Context Protocol)","description":"A comprehensive guide on how to integrate OpenAI\'s SDK with MCP (Model Context Protocol) to enhance AI model interactions through dynamic tool calling.","date":"2025-10-09T12:02:09.000Z","tags":[{"inline":false,"label":"MCP","permalink":"/tags/mcp","description":"MCP is an open protocol for connecting LLM apps to external data sources and tools, enabling seamless integration and interoperability.\\n"},{"inline":false,"label":"OpenAI","permalink":"/tags/openai","description":"OpenAI is an AI research and deployment company focused on ensuring that artificial general intelligence benefits all of humanity.\\n"},{"inline":false,"label":"Tool Calling","permalink":"/tags/tool-calling","description":"Tool calling refers to the ability of AI models to invoke external tools or APIs to perform specific tasks or retrieve information, enhancing their functionality and capabilities.\\n"},{"inline":false,"label":"OAS","permalink":"/tags/oas","description":"OpenAPI Specification\u2014industry standard for describing RESTful APIs and enabling interoperability."}],"readingTime":5.88,"hasTruncateMarker":true,"authors":[{"name":"Adrian Escutia","title":"La Rebelion Founder","url":"https://adrian.escutia.me","page":{"permalink":"/authors/adrian"},"socials":{"x":"https://x.com/ades_mx","linkedin":"https://www.linkedin.com/in/adrianescutia/","github":"https://github.com/adrianescutia","newsletter":"https://rebelion.la"},"bio":"Adrian is the founder of La Rebelion, a newsletter about AI, technology, and the future. He is also a software engineer and entrepreneur.","imageURL":"https://github.com/adrianescutia.png","key":"adrian"}],"frontMatter":{"title":"OpenAI SDK Meets MCP (Model Context Protocol)","description":"A comprehensive guide on how to integrate OpenAI\'s SDK with MCP (Model Context Protocol) to enhance AI model interactions through dynamic tool calling.","authors":["adrian"],"tags":["mcp","openai","tool-calling","oas"],"image":"https://cdn.gamma.app/z3n406kubdfbysb/generated-images/ibH5HhVl1c3OZPUlUBXBp.png"},"unlisted":false,"nextItem":{"title":"MCP Threats and Misleadings - Prompt Injection","permalink":"/mcp-threats-prompt-injection"}},"content":"import ReactPlayer from \'react-player\'\\n\\nAn evolution of tool calling with MCP, thanks to OpenAI\'s latest SDK updates.\\n\\nStep-by-step, I\'ll guide you through setting up an MCP server, integrating it with the OpenAI SDK, and running a complete example that showcases **dynamic tool calling**. By the end of this post, you\'ll be equipped to leverage MCP in your own OpenAI-powered applications.\\n\\nEnd-to-End Example, Setting Up an MCP Server, Integrating with OpenAI LLM, and Running some tests to see it in action.\\n\\n\x3c!-- truncate --\x3e\\n\\nMCP, or Model Context Protocol, is a framework that enhances how AI models interact with their operational context. By integrating MCP, you ensure your AI models are not only aware of the data they process but also the environment in which they operate. This leads to more accurate and contextually relevant outputs.\\n\\n## Why Integrate OpenAI with MCP?\\n\\nOpenAI\'s powerful language models take tool calling to the next level when you leverage MCP. This integration enables dynamic interactions, letting you build applications that adapt to various scenarios and user needs. I explained [how MCP tool calling works](/oas-v4-is-out#how-does-mcp-work-for-mere-mortals) in a previous post. Now, I\'ll show you how OpenAI\'s implementation impressed me.\\n\\nWith a simple, clever setup, you create applications that understand user queries in natural language and fetch relevant tools from MCP servers. This is extremely powerful\u2014it enables seamless, efficient interactions with AI models, making them more useful in real-world applications. Imagine an automated customer-support bot using MCP: it instantly retrieves necessary tools from MCP servers to resolve complex queries, such as billing discrepancies or technical troubleshooting, on the first call. This opens new possibilities for building intelligent applications that respond to user needs with greater precision and context-awareness.\\n\\nAnthropic\'s Claude Desktop app (MCP Client) was groundbreaking almost a year ago. Early adopters were impressed, and now OpenAI seamlessly integrates these capabilities into their SDKs. As the Spanish saying goes, \\"Nadie sabe para quien trabaja\\" (Nobody knows for whom they work), but MCP is clearly shaping the future of AI applications. OpenAI is making significant strides by implementing Anthropic\'s legacy directly in the LLM.\\n\\nYou can build smarter, more adaptive applications by letting AI models fetch and use tools from MCP servers, reducing manual integration and boosting context-awareness.\\n\\n:::note\\nThe OpenAI\'s MCP integration is currently in beta, so expect some rough edges. However, the potential is immense, and I\'m excited to see how this evolves.\\n:::\\n\\n## Three Approaches to Tool Calling\\n\\nYou have three main ways to implement tool calling:\\n\\n| Approach                   | Description                                                          | Key Strength                                  |\\n| -------------------------- | -------------------------------------------------------------------- | --------------------------------------------- |\\n| **MCP Clients**            | Intermediary applications bridging models and external tool servers. | Dynamic retrieval and adaptation              |\\n| **Direct Tool Calling**    | LLMs with built-in function calling via structured JSON.             | Precise programmatic control                  |\\n| **Native MCP Integration** | OpenAI\u2019s latest approach combining both worlds.                      | Automatic discovery and zero-config operation |\\n\\n\\nWhen choosing, remember: Direct Tool Calling is easy to set up but requires manual tool management. MCP Clients offer dynamic adaptability, fetching tools as needed and calling them for the user. LLMs with Native MCP Support provide the most seamless experience, automatically integrating and using tools from MCP Servers, though you may need some upfront knowledge about OpenAI\'s system and SDKs.\\n\\n**Summary:**  \\nSelect the approach that best fits your workflow\u2014manual, dynamic, or fully automated tool integration.\\n\\n:::note\\nThe OpenAI\'s MCP integration may not be call by the model directly; my guess is that it uses an internal MCP Client to fetch and call the tools, similar to how [QBot](https://docs.mcp.com.ai/components/qbot) works.\\n:::\\n\\n## End-to-End Example\\n\\nQuick check:\\n\\n```bash\\ncurl -X POST \\"https://api.openai.com/v1/responses\\" \\\\\\n -H \\"Authorization: Bearer $OPENAI_API_KEY\\" \\\\\\n -H \\"Content-Type: application/json\\" \\\\\\n -d \'{\\n \\"model\\": \\"gpt-4.1\\",\\n \\"tools\\": [\\n {\\n \\"type\\": \\"mcp\\",\\n \\"server_label\\": \\"tacosmcp\\",\\n \\"server_url\\": \\"https://tacostore.run.mcp.com.ai\\",\\n \\"require_approval\\": \\"never\\"\\n }\\n ],\\n \\"input\\": \\"What tacos do you have in the menu?\\"\\n }\'\\n```\\n\\nLet\'s walk through the process: set up an MCP server, integrate the OpenAI SDK, write the code, run it, and see the results. These steps help you use MCP to enhance your applications.\\n\\n**Summary:**  \\nFollow these steps to quickly connect OpenAI models to MCP and unlock dynamic tool calling.\\n\\n### 1. Set Up an MCP Server\\n\\nFirst, set up an MCP server hosting the tools you want to use. You can run your own MCP server or use an existing one. For this example, use the HAPI MCP Server\u2014a simple way to get started. Find the [HAPI MCP Server docs here](https://docs.mcp.com.ai/components/hapi-server/).\\n\\nOpen a terminal, ensure Bun is installed, and run:\\n\\n```bash\\n# Start a HAPI MCP Server\\nbun dev serve tacos --headless\\n```\\n\\nThis starts a local MCP server hosting a tacos online store at `http://localhost:3000`. I asked [Postman Agent Builder](https://www.postman.com/product/ai-agent-builder/) to create a simple tacos store API, which I used for the MCP server.\\n\\n### 2. Create an OpenAI Account\\n\\nIf you don\'t have one, sign up for an OpenAI account and get your API key at the [OpenAI platform](https://platform.openai.com/).\\n\\n### 3. Install the OpenAI SDK\\n\\nUse the OpenAI SDK in your preferred language. For this example, use JavaScript and TypeScript with Bun. In another terminal, run:\\n\\n```bash\\n# Initialize a new Bun project\\nbun init -y -m\\n# Install the OpenAI SDK\\nbun add openai\\n```\\n\\n### 4. Write the Code\\n\\nCreate a new file, for example, `index.js`, and add:\\n\\n```javascript\\nimport OpenAI from \\"openai\\"\\n\\nconst client = new OpenAI({\\n  apiKey: process.env[\'OPENAI_API_KEY\'], // This is the default and can be omitted\\n})\\n\\nconst resp = await client.responses.create({\\n  model: \\"gpt-5\\",\\n  tools: [\\n    {\\n      type: \\"mcp\\",\\n      server_label: \\"tacosmcp\\",\\n      server_description: \\"The Tacos MCP Client\\",\\n      server_url: \\"https://tacostore.run.mcp.com.ai\\",\\n      require_approval: \\"never\\",\\n    },\\n  ],\\n  input: \\"What tacos do you have in the menu?\\",\\n})\\n\\nconsole.log(resp.output_text)\\n```\\n\\n### 5. Run the Code\\n\\nSet your OpenAI API key in the `OPENAI_API_KEY` environment variable, then run:\\n\\n```bash\\nbun run index.js\\n```\\n\\n### 6. See the Magic\\n\\nThe model fetches tools from the MCP server and uses them to answer your query. You should see a response listing the tacos available on the menu. To reinforce your learning, validate the returned taco list against the API response. This simple verification ensures tool execution works correctly and builds confidence in using MCP.\\n\\n### 7. Explore Further\\n\\nModify the input query to ask different questions or explore other tools on the MCP server. The possibilities are endless!\\n\\n**Summary:**  \\nYou can set up, connect, and validate MCP-powered tool calling in minutes, then experiment to discover more capabilities.\\n\\n## Demo Video\\n\\n[PLACEHOLDER] - video coming soon\\n\x3c!-- <ReactPlayer\\n  src=\'https://youtu.be/\'\\n  style={{ width: \'90%\', height: \'auto\', aspectRatio: \'4/3\' }}\\n  controls\\n/> --\x3e\\n\\n## Conclusion\\n\\nIntegrating OpenAI with MCP unlocks a world of possibilities for building intelligent applications that adapt to various scenarios and user needs. By leveraging MCP, you create dynamic, context-aware interactions with AI models, leading to more accurate and relevant outputs.\\n\\n**Summary:**  \\nUse MCP to make your AI applications smarter, more flexible, and better suited to real-world challenges.\\n\\n## References\\n\\n- [OpenAI API Documentation](https://platform.openai.com/docs/)\\n- [Connectors and MCP servers (Beta)](https://platform.openai.com/docs/guides/tools-connectors-mcp?lang=javascript)\\n- [OpenAI NPM Package](https://www.npmjs.com/package/openai)\\n- [Model Context Protocol (MCP) Overview](https://modelcontextprotocol.io/)"},{"id":"/mcp-threats-prompt-injection","metadata":{"permalink":"/mcp-threats-prompt-injection","source":"@site/blog/mcp-threats-prompt-injection.md","title":"MCP Threats and Misleadings - Prompt Injection","description":"An overview of threats and misleadings related to prompt injection in the context of MCP (Model Context Protocol).","date":"2025-10-08T11:36:31.000Z","tags":[{"inline":false,"label":"MCP","permalink":"/tags/mcp","description":"MCP is an open protocol for connecting LLM apps to external data sources and tools, enabling seamless integration and interoperability.\\n"},{"inline":false,"label":"Security","permalink":"/tags/security","description":"Security involves protecting systems, networks, and data from digital attacks, unauthorized access, and damage\\n"}],"readingTime":3.15,"hasTruncateMarker":true,"authors":[{"name":"Adrian Escutia","title":"La Rebelion Founder","url":"https://adrian.escutia.me","page":{"permalink":"/authors/adrian"},"socials":{"x":"https://x.com/ades_mx","linkedin":"https://www.linkedin.com/in/adrianescutia/","github":"https://github.com/adrianescutia","newsletter":"https://rebelion.la"},"bio":"Adrian is the founder of La Rebelion, a newsletter about AI, technology, and the future. He is also a software engineer and entrepreneur.","imageURL":"https://github.com/adrianescutia.png","key":"adrian"}],"frontMatter":{"title":"MCP Threats and Misleadings - Prompt Injection","description":"An overview of threats and misleadings related to prompt injection in the context of MCP (Model Context Protocol).","authors":["adrian"],"tags":["mcp","security"],"image":"https://cdn.gamma.app/z3n406kubdfbysb/generated-images/trv8ZioPQIQ84iK46Bd8B.png"},"unlisted":false,"prevItem":{"title":"OpenAI SDK Meets MCP (Model Context Protocol)","permalink":"/openai-meets-mcp"},"nextItem":{"title":"Swagger/OAS v4 Is out","permalink":"/oas-v4-is-out"}},"content":"Prompt injection is a critical security risk for any system using large language models (LLMs), including those built with Model Context Protocol (MCP). You must understand how prompt injection works, why MCP cannot prevent it, and what steps you should take to protect your users and applications (MCP Clients).\\n\\n\x3c!-- truncate --\x3e\\n\\n## Introduction\\n\\nMCP enables users and clients to discover and pull prompts from MCP Servers. This flexibility means you, as a client developer, are responsible for validating and sanitizing all user inputs before they reach the LLM. MCP Clients act as intermediaries between MCP Servers, end users, and LLMs. If you do not implement proper validation, malicious prompts can reach the LLM and cause harmful or unintended outputs.\\n\\n:::warning\\nPrompt injection is recognized as a top risk by OpenAI ([OpenAI Prompt Injection Guide](https://platform.openai.com/docs/guides/prompt-injection)), Anthropic ([Anthropic Prompt Injection FAQ](https://www.anthropic.com/index/prompt-injection)), and OWASP ([OWASP AI Security Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/)).\\n:::\\n\\n## How Prompt Injection Works in MCP\\n\\n```mermaid\\nflowchart RL\\n  A[Client Developer] --\x3e|Builds| E[MCP Client]\\n  E --\x3e|Connects to| B[MCP Server]\\n  B --\x3e|Serves Prompts| E\\n  E --\x3e|Delivers Prompts| C[End User]\\n  C --\x3e|Inputs/Queries| E\\n  E --\x3e|Forwards Inputs| D[LLM]\\n  D --\x3e|Generates Responses| E\\n  E --\x3e|Returns Responses| C\\n  B --\x3e|Provides Prompts| E\\n  subgraph Perspectives\\n    E\\n    C\\n    D\\n  end\\n```\\n\\n**Key Roles:**\\n- **Client Developer:** You build MCP Clients and must implement security and validation.\\n- **MCP Client:** Your app acts as an intermediary, handling prompt delivery and user input forwarding.\\n- **End User:** Users interact with LLMs via MCP Clients and may provide inputs that could be exploited.\\n- **LLM:** Processes prompts and user inputs, vulnerable to prompt injection if upstream validation is insufficient.\\n\\n## Threats\\n\\nPrompt injection can lead to several serious risks:\\n\\n- **Malicious Prompts:** Attackers craft prompts to manipulate the LLM into generating harmful or unintended outputs ([OpenAI](https://platform.openai.com/docs/guides/prompt-injection)).\\n- **User Input Manipulation:** End users may input data designed to exploit vulnerabilities in the LLM\'s response generation ([Anthropic](https://www.anthropic.com/index/prompt-injection)).\\n- **Data Leakage:** Sensitive information may be exposed through manipulated prompts or responses ([NIST AI RMF](https://airmf.nist.gov/)).\\n- **Reputation Damage:** Misleading outputs generated by LLMs due to prompt injection can harm your application\'s reputation.\\n\\n## Common Misleadings\\n\\nAvoid these misconceptions:\\n\\n- **False Sense of Security:** MCP does not provide inherent protection against prompt injection. You must implement your own safeguards.\\n- **Overreliance on LLMs:** LLMs do not automatically handle all types of inputs safely. Validation is essential.\\n- **Misunderstanding Roles:** Security measures must be implemented in the MCP Client, not just the server or LLM.\\n- **Assumption of Trustworthiness:** Do not trust all prompts from third-party MCP Servers. Use official or self-hosted servers when possible ([OWASP](https://owasp.org/www-project-top-10-for-large-language-model-applications/)).\\n- **Neglecting Client Responsibility:** Input validation is your responsibility as the MCP Client developer.\\n\\n## Mitigation Strategies\\n\\nFollow these best practices to reduce prompt injection risks:\\n\\n1. **Input Validation:** Check all user inputs for malicious content before forwarding to the LLM ([OWASP](https://owasp.org/www-project-top-10-for-large-language-model-applications/)).\\n2. **Sanitization:** Remove or neutralize potentially harmful elements in user inputs.\\n3. **User Education:** Inform users about prompt injection risks and safe input practices.\\n4. **Regular Audits:** Audit your MCP Client regularly to identify and fix vulnerabilities.\\n5. **Monitoring and Logging:** Track interactions and flag suspicious activities for review.\\n\\n## References\\n\\n- [OpenAI: Prompt Injection Guide](https://platform.openai.com/docs/guides/prompt-injection)\\n- [Anthropic: Prompt Injection FAQ](https://www.anthropic.com/index/prompt-injection)\\n- [OWASP: Top 10 for LLM Applications](https://owasp.org/www-project-top-10-for-large-language-model-applications/)\\n- [NIST: AI Risk Management Framework](https://airmf.nist.gov/)\\n\\nBy understanding and addressing these threats, you can secure your MCP Client against prompt injection vulnerabilities and protect your users and reputation."},{"id":"/oas-v4-is-out","metadata":{"permalink":"/oas-v4-is-out","source":"@site/blog/oas-v4-is-out.mdx","title":"Swagger/OAS v4 Is out","description":"Exploring the potential of OpenAPI Specification (OAS) v4 for AI tool integration and how it can complement Model Context Protocol (MCP).","date":"2025-10-08T11:36:31.000Z","tags":[{"inline":false,"label":"OAS","permalink":"/tags/oas","description":"OpenAPI Specification\u2014industry standard for describing RESTful APIs and enabling interoperability."},{"inline":false,"label":"MCP","permalink":"/tags/mcp","description":"MCP is an open protocol for connecting LLM apps to external data sources and tools, enabling seamless integration and interoperability.\\n"},{"inline":false,"label":"API","permalink":"/tags/api","description":"An API (Application Programming Interface) is a set of rules and protocols that allows different software applications to communicate and interact with each other.\\n"},{"inline":false,"label":"AI","permalink":"/tags/ai","description":"Artificial Intelligence (AI) is the simulation of human intelligence in machines that are programmed to think and learn like humans.\\n"}],"readingTime":9.81,"hasTruncateMarker":true,"authors":[{"name":"Adrian Escutia","title":"La Rebelion Founder","url":"https://adrian.escutia.me","page":{"permalink":"/authors/adrian"},"socials":{"x":"https://x.com/ades_mx","linkedin":"https://www.linkedin.com/in/adrianescutia/","github":"https://github.com/adrianescutia","newsletter":"https://rebelion.la"},"bio":"Adrian is the founder of La Rebelion, a newsletter about AI, technology, and the future. He is also a software engineer and entrepreneur.","imageURL":"https://github.com/adrianescutia.png","key":"adrian"}],"frontMatter":{"title":"Swagger/OAS v4 Is out","description":"Exploring the potential of OpenAPI Specification (OAS) v4 for AI tool integration and how it can complement Model Context Protocol (MCP).","authors":["adrian"],"tags":["oas","mcp","api","ai"],"image":"/img/ai-friend-chatting.png"},"unlisted":false,"prevItem":{"title":"MCP Threats and Misleadings - Prompt Injection","permalink":"/mcp-threats-prompt-injection"}},"content":"import ReactPlayer from \'react-player\'\\n\\n\ud83d\udea8 \\"OpenAPI Specification (OAS) v4 is out\\" - *That I wish*, this is the kind of headline I would expect to see soon, because OAS can easily be extended to enable RESTful APIs work seamlessly with AI. \\n\\nBy the end of this article, you\'ll know how to let any LLM call your REST tools automatically using OAS.\\n\\n\x3c!-- truncate --\x3e\\n\\n**What Does It Mean for AI Tool Integration?** Let\'s explore how OAS v4 could fit into the AI landscape, complementing protocols like Model Context Protocol (MCP). \\n\\n:::note  \\nThis is a speculative article about what I would like to see in OAS v4, based on my experience with Model Context Protocol (MCP) and AI tool integration. OAS v4 is not yet released, and this article is not endorsed by the OpenAPI Initiative or any other organization.  \\n:::\\n\\nOver the past decade, OAS has become the standard for describing RESTful APIs and is widely adopted across various industries. Now, it\'s time to expand it to cover AI use cases, addressing the evolving needs of developers.\\n\\nModel Context Protocol (MCP) has recently gained a lot of attention. It\'s great for local integrations, allows you to adjust the MCP server for backend connections, and with the latest updates, it can now handle remote calls, moving closer to RESTful APIs. \\n\\nSince the beginning [MCP was designed for local integrations mainly](https://rebelion.la/model-context-protocol-mcp-is-it-a-protocol-or-a-contract#heading-the-lsp-connection-understanding-mcps-roots). The breakthrough of MCP lies in its **ability to let the LLM discover the tools available**. This tool discovery feature marks MCP\'s unique value proposition, transforming the way we approach AI integrations. From there, everything falls into place similar to RESTful APIs: the client acts as a lightweight orchestration layer, the server handles the backend tasks, and the LLM plays the role of the brain, deciding which tools to use and how to use them.\\n\\n## How does MCP work? For mere mortals\\n\\nI have noticed that many people struggle to understand how MCP works, so let me try to explain it in a simple way.\\n\\nLet\'s imagine a conversation between the client (you), the LLM (a friend), and the server (weather guy):\\n\\nSetting the scene: you have a person (the MCP Server) who speaks a language you don\'t understand, such as French, and you have a friend who speaks both languages (The LLM), French and English. You\'d like to know the weather in Paris, so you can ask your friend for help. Your friend asks the person in French, gets the answer, and translates it back to you in English. In this scenario, your friend represents the LLM supporting tool calling natively. \\n\\nNow, imagine you can write in French but don\'t speak it fluently. You\'d like to know the weather in Paris, but this time you need someone to guide you on what to ask in French. You write the message asking for the weather. Once you receive the answer, you can translate it without help. This scenario reflects the role of an LLM that doesn\'t support tool calling natively, and the client has to call the tool directly.\\n\\nComplex, right? Let\'s see it in a sequence diagram:\\n\\n```mermaid\\nsequenceDiagram\\n  participant U as User\\n  participant C as Client\\n  participant L as LLM\\n  participant S as Server\\n  U->>C: User input<br/>(e.g., \\"What tacos do you have in the menu?\\")\\n  C->>S: Fetch API spec (tools/list)\\n  S--\x3e>C: API spec (tools [list])\\n  alt LLM supports tool calling\\n    C->>L: API spec (tools [list])\\n    C->>L: User input & context<br/>(user preference, etc)\\n    L--\x3e>C: Tool selection<br/>(assistant suggests a function call)\\n    C->>S: API call\\n  else LLM does not support tool calling\\n    C->>L: User input, tools [list] & context<br/>(user preference, etc)\\n    L--\x3e>C: Tool selection\\n    C->>S: API call<br/>(Client decides protocol, auth, etc)\\n  end\\n  S--\x3e>C: API response\\n  C--\x3e>L: API response\\n  L--\x3e>C: Result to user\\n  C--\x3e>U: Final response\\n```\\n\\nThe LLM receives a prompt with a description of the [available tools](https://modelcontextprotocol.io/specification/2025-06-18/server/tools#listing-tools). This tool list can be [mapped from the OpenAPI spec](https://rebelion.la/you-dont-need-to-implement-mcp-servers-a-contract-first-approach-to-ai-tool-integration?showSharer=true#heading-example-conversion-openapi-mcp). The LLM then chooses which tool to use and how to use it. The client simply passes the user input to the LLM, which decides what to do next. Some LLMs, like GPT-4-turbo and Llama2, support tool calling natively. Others, like Claude, do not, so the client calls the tool directly, based on the LLM\'s instruction.\\n\\nAs you can see, either the LLM supports tool calling natively or not, the flow is similar. The only difference is that in the latter case, the client does the call to the server, and forwards the response to the LLM to get the final result for the end-user - based on the LLM\'s instruction.\\n\\n## OAS v4 - The missing piece\\n\\nThe missing piece in OAS compared to MCP is that OAS does not have a discovery mechanism for the LLM to know which tools are available. This is where OAS v4 could shine, by adding a way for the LLM to discover the API spec and the available tools.\\n\\nImagine if OAS v4 had a way to describe the tools available in a way that the LLM could understand, and then the LLM could decide which tool to use, and how to use it. This would make it possible for any LLM to work seamlessly with RESTful APIs, without the need for a separate protocol like MCP, or even better, MCP could be used as a contract-first approach to AI tool integration, where the OAS v4 spec is the contract that defines the tools available, and the MCP server implements the backend logic. That\'s precisely what I have been advocating for a while now with my [contract-first approach to AI tool integration](https://rebelion.la/you-dont-need-to-implement-mcp-servers-a-contract-first-approach-to-ai-tool-integration?showSharer=true) and the *Headless API* (HAPI) initiative for MCP.\\n\\nNow that I have explained how MCP works, and how OAS v4 could be the missing piece, I hope you can see the potential of OAS v4 for AI tool integration. But, don\'t stop here, next is what I would like to see in OAS v4.\\n\\n## What You Should Expect in OAS v4\\n\\nOAS v4 is not just an incremental update\u2014it\'s an opportunity to rethink how APIs and AI tools work together. Here is what you should expect and advocate for in the next version, based on best practices and the needs of modern AI integrations.\\n\\n## Modular, Multi-File API Specifications\\n\\nYou need modular specs for real-world APIs. OAS v4 should support multi-file specifications natively, allowing you to break down large APIs into smaller, reusable modules (for example, pets, users, orders). This approach enhances maintainability and collaboration across teams. Instead of a single massive file, you organize your API into logical domains:\\n\\n- **API Root:** Main file with info, servers, and tags.\\n- **Security:** Dedicated files for OAuth2 and other schemes.\\n- **Paths by Domain:** Split endpoints into logical groups (e.g. `pet.api.yaml`, `user.api.yaml`).\\n- **Components/Models:** Reusable schemas separated by domain.\\n\\nThis modular structure keeps specs manageable (500\u20131000 lines each) and enables code generators and clients to see one unified spec. Tools like [Redocly CLI](https://redocly.com/docs/cli/file-management#one-large-file-to-many-small-ones) already offer splitting, but OAS v4 should standardize it. Modular specs also help AI tools (MCP servers) consume only the relevant modules, **improving performance and reducing cognitive load for LLMs** that may have context length limitations and make the LLMs\' job easier when deciding which tools to use.\\n\\n:::info\\nThe current `$ref` approach only allows referencing individual components, not entire files. Modular specs let you load what you need, when you need it.\\n:::\\n\\n**Benefits:**\\n- Parallel development and reviews\\n- Domain-specific clients (AI agents load only relevant modules)\\n- Easier maintenance and updates\\n- Improved collaboration\\n- Better tooling support\\n- Granular security schemes\\n\\n## Well-Known Tools Manifest for AI Discovery\\n\\nAI clients must discover available tools easily. OAS v4 should adopt a well-known URI convention (such as `/.well-known/mcp/tools-manifest.json`) so MCP clients and AI agents can auto-discover your API\'s tools. This follows patterns from [OpenID Connect Discovery](https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderConfig) and [OAuth 2.0 Security Scheme](https://spec.openapis.org/oas/v3.2.0.html#security-scheme-object). Give agents a predictable URL to learn your API\u2014this small change pays off big in discoverability.\\n\\n## AI-First Annotations and Metadata\\n\\nOAS v4 should introduce AI-first annotations to help clients and LLMs use APIs effectively. Add metadata extensions (such as `x-llm-hint`, `x-llm-example`) to endpoints, parameters, and responses. Use the [`kind`](https://spec.openapis.org/registry/tag-kind/) property to categorize tags for AI relevance. For example:\\n\\n```yaml\\ntags:\\n  - name: contacts\\n    summary: Manage contacts\\n    description: Endpoints for creating, updating, and searching contacts\\n    kind: ai-tool\\n\\n  - name: internal\\n    summary: Internal admin endpoints\\n    description: Endpoints not intended for AI or public use\\n    kind: internal\\n```\\n\\nWith this approach, AI agents and MCP servers filter and discover only the endpoints relevant for tool calling, ignoring internal or non-AI endpoints. You can further extend this pattern with custom extensions:\\n\\n```yaml\\npaths:\\n  /contacts/search:\\n    get:\\n      tags: [contacts]\\n      x-llm-hint: \\"Use this endpoint to search for contacts by name or email.\\"\\n      x-llm-example: \\"Find all contacts named Alice.\\"\\n```\\n\\nThese annotations guide LLMs on how to use specific endpoints, making your API more AI-friendly and discoverable.\\n\\n## References\\n\\n- [OpenAPI Specification](https://spec.openapis.org/oas/latest.html)\\n- [Redocly CLI: File Management](https://redocly.com/docs/cli/file-management#one-large-file-to-many-small-ones)\\n- [OpenID Connect Discovery](https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderConfig)\\n- [OWASP AI Security Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/)\\n\\nBy advocating for these features, you help shape OAS v4 into a standard that supports scalable, AI-ready APIs for the next generation of applications.\\n\\n## Example: Conversion OpenAPI \u2192 MCP\\n\\nHere is a simple example of how an OpenAPI spec can be converted to an MCP tools manifest.\\n\\nUsing the [Petstore example](https://petstore3.swagger.io/api/v3/openapi.json) from Swagger, we can extract the relevant information to create an MCP tools manifest. Deploying an MCP server with the [`hapi` server](https://docs.mcp.com.ai/components/hapi-server/), we can extend the OpenAPI spec with AI-first annotations and modular structure.\\n\\nIn the demo below, the Petstore API is extended to integrate WorkOS for authentication, and the OpenAPI spec is modularized into separate files for better organization.\\n\\n```yaml\\napiVersion: mcp.com.ai/v1\\nkind: Security\\nmetadata:\\n  name: hapi-security-config\\n  namespace: demo\\n  labels:\\n    app: demo\\n  annotations:\\n    description: HAPI server security configuration\\n    owner: team-hapi\\n    environment: development\\n    version: v1.0.0\\nspec:\\n  cors:\\n    enabled: true\\n    origin:\\n      - http://localhost:8080 # Your MCP Client\\n      - http://localhost:6274 # MCP Inspector\\n    headers:\\n      - Content-Type\\n      - Authorization\\n      - X-Requested-With\\n    methods:\\n      - GET\\n      - POST\\n      - PUT\\n      - DELETE\\n      - OPTIONS\\n  security:\\n    demo_auth:\\n      client_id: client_0\\n      well-known: https://*****.app/.well-known/oauth-authorization-server\\n  securitySchemes:\\n    demo_auth:\\n      type: oauth2\\n      flows:\\n        authorizationCode:\\n          authorizationUrl: https://api.workos.com/sso/authorize?connection=conn_01\\n          tokenUrl: https://*****.app/oauth2/token\\n          scopes:\\n            read: Read\\n            write: Modify\\n            admin: Access to admin operations\\n            read_all: Read private resources\\n```\\n\\n<ReactPlayer\\n  src=\'https://youtu.be/S2_Z0rbnOH8\'\\n  style={{ width: \'90%\', height: \'auto\', aspectRatio: \'4/3\' }}\\n  controls\\n/>\\n\\n## Conclusion\\n\\nOAS v4 has the potential to revolutionize how AI systems interact with RESTful APIs. By incorporating modular specs, AI-first annotations, and well-known discovery endpoints, OAS v4 can make it easier for LLMs to understand and utilize APIs effectively. This would not only benefit developers but also pave the way for more seamless AI integrations across various applications."}]}}')}}]);