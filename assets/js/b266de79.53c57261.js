"use strict";(self.webpackChunkmcp_ai=self.webpackChunkmcp_ai||[]).push([[3518],{4369:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/mcp-threats-prompt-injection","metadata":{"permalink":"/mcp-threats-prompt-injection","source":"@site/blog/mcp-threats-prompt-injection.md","title":"MCP Threats and Misleadings - Prompt Injection","description":"An overview of threats and misleadings related to prompt injection in the context of MCP (Model Context Protocol).","date":"2025-10-08T11:36:31.000Z","tags":[{"inline":false,"label":"MCP","permalink":"/tags/mcp","description":"MCP is an open protocol for connecting LLM apps to external data sources and tools, enabling seamless integration and interoperability.\\n"},{"inline":false,"label":"Security","permalink":"/tags/security","description":"Security involves protecting systems, networks, and data from digital attacks, unauthorized access, and damage\\n"}],"readingTime":3.15,"hasTruncateMarker":true,"authors":[{"name":"Adrian Escutia","title":"La Rebelion Founder","url":"https://adrian.escutia.me","page":{"permalink":"/authors/adrian"},"socials":{"x":"https://x.com/ades_mx","linkedin":"https://www.linkedin.com/in/adrianescutia/","github":"https://github.com/adrianescutia","newsletter":"https://rebelion.la"},"bio":"Adrian is the founder of La Rebelion, a newsletter about AI, technology, and the future. He is also a software engineer and entrepreneur.","imageURL":"https://github.com/adrianescutia.png","key":"adrian"}],"frontMatter":{"title":"MCP Threats and Misleadings - Prompt Injection","description":"An overview of threats and misleadings related to prompt injection in the context of MCP (Model Context Protocol).","authors":["adrian"],"tags":["mcp","security"]},"unlisted":false,"nextItem":{"title":"Swagger/OAS v4 Is out","permalink":"/oas-v4-is-out"}},"content":"Prompt injection is a critical security risk for any system using large language models (LLMs), including those built with Model Context Protocol (MCP). You must understand how prompt injection works, why MCP cannot prevent it, and what steps you should take to protect your users and applications (MCP Clients).\\n\\n\x3c!-- truncate --\x3e\\n\\n## Introduction\\n\\nMCP enables users and clients to discover and pull prompts from MCP Servers. This flexibility means you, as a client developer, are responsible for validating and sanitizing all user inputs before they reach the LLM. MCP Clients act as intermediaries between MCP Servers, end users, and LLMs. If you do not implement proper validation, malicious prompts can reach the LLM and cause harmful or unintended outputs.\\n\\n:::warning\\nPrompt injection is recognized as a top risk by OpenAI ([OpenAI Prompt Injection Guide](https://platform.openai.com/docs/guides/prompt-injection)), Anthropic ([Anthropic Prompt Injection FAQ](https://www.anthropic.com/index/prompt-injection)), and OWASP ([OWASP AI Security Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/)).\\n:::\\n\\n## How Prompt Injection Works in MCP\\n\\n```mermaid\\nflowchart RL\\n  A[Client Developer] --\x3e|Builds| E[MCP Client]\\n  E --\x3e|Connects to| B[MCP Server]\\n  B --\x3e|Serves Prompts| E\\n  E --\x3e|Delivers Prompts| C[End User]\\n  C --\x3e|Inputs/Queries| E\\n  E --\x3e|Forwards Inputs| D[LLM]\\n  D --\x3e|Generates Responses| E\\n  E --\x3e|Returns Responses| C\\n  B --\x3e|Provides Prompts| E\\n  subgraph Perspectives\\n    E\\n    C\\n    D\\n  end\\n```\\n\\n**Key Roles:**\\n- **Client Developer:** You build MCP Clients and must implement security and validation.\\n- **MCP Client:** Your app acts as an intermediary, handling prompt delivery and user input forwarding.\\n- **End User:** Users interact with LLMs via MCP Clients and may provide inputs that could be exploited.\\n- **LLM:** Processes prompts and user inputs, vulnerable to prompt injection if upstream validation is insufficient.\\n\\n## Threats\\n\\nPrompt injection can lead to several serious risks:\\n\\n- **Malicious Prompts:** Attackers craft prompts to manipulate the LLM into generating harmful or unintended outputs ([OpenAI](https://platform.openai.com/docs/guides/prompt-injection)).\\n- **User Input Manipulation:** End users may input data designed to exploit vulnerabilities in the LLM\'s response generation ([Anthropic](https://www.anthropic.com/index/prompt-injection)).\\n- **Data Leakage:** Sensitive information may be exposed through manipulated prompts or responses ([NIST AI RMF](https://airmf.nist.gov/)).\\n- **Reputation Damage:** Misleading outputs generated by LLMs due to prompt injection can harm your application\'s reputation.\\n\\n## Common Misleadings\\n\\nAvoid these misconceptions:\\n\\n- **False Sense of Security:** MCP does not provide inherent protection against prompt injection. You must implement your own safeguards.\\n- **Overreliance on LLMs:** LLMs do not automatically handle all types of inputs safely. Validation is essential.\\n- **Misunderstanding Roles:** Security measures must be implemented in the MCP Client, not just the server or LLM.\\n- **Assumption of Trustworthiness:** Do not trust all prompts from third-party MCP Servers. Use official or self-hosted servers when possible ([OWASP](https://owasp.org/www-project-top-10-for-large-language-model-applications/)).\\n- **Neglecting Client Responsibility:** Input validation is your responsibility as the MCP Client developer.\\n\\n## Mitigation Strategies\\n\\nFollow these best practices to reduce prompt injection risks:\\n\\n1. **Input Validation:** Check all user inputs for malicious content before forwarding to the LLM ([OWASP](https://owasp.org/www-project-top-10-for-large-language-model-applications/)).\\n2. **Sanitization:** Remove or neutralize potentially harmful elements in user inputs.\\n3. **User Education:** Inform users about prompt injection risks and safe input practices.\\n4. **Regular Audits:** Audit your MCP Client regularly to identify and fix vulnerabilities.\\n5. **Monitoring and Logging:** Track interactions and flag suspicious activities for review.\\n\\n## References\\n\\n- [OpenAI: Prompt Injection Guide](https://platform.openai.com/docs/guides/prompt-injection)\\n- [Anthropic: Prompt Injection FAQ](https://www.anthropic.com/index/prompt-injection)\\n- [OWASP: Top 10 for LLM Applications](https://owasp.org/www-project-top-10-for-large-language-model-applications/)\\n- [NIST: AI Risk Management Framework](https://airmf.nist.gov/)\\n\\nBy understanding and addressing these threats, you can secure your MCP Client against prompt injection vulnerabilities and protect your users and reputation."},{"id":"/oas-v4-is-out","metadata":{"permalink":"/oas-v4-is-out","source":"@site/blog/oas-v4-is-out.mdx","title":"Swagger/OAS v4 Is out","description":"Exploring the potential of OpenAPI Specification (OAS) v4 for AI tool integration and how it can complement Model Context Protocol (MCP).","date":"2025-10-08T11:36:31.000Z","tags":[{"inline":false,"label":"OAS","permalink":"/tags/oas","description":"OpenAPI Specification\u2014industry standard for describing RESTful APIs and enabling interoperability."},{"inline":false,"label":"MCP","permalink":"/tags/mcp","description":"MCP is an open protocol for connecting LLM apps to external data sources and tools, enabling seamless integration and interoperability.\\n"},{"inline":false,"label":"API","permalink":"/tags/api","description":"An API (Application Programming Interface) is a set of rules and protocols that allows different software applications to communicate and interact with each other.\\n"},{"inline":false,"label":"AI","permalink":"/tags/ai","description":"Artificial Intelligence (AI) is the simulation of human intelligence in machines that are programmed to think and learn like humans.\\n"}],"readingTime":9.59,"hasTruncateMarker":true,"authors":[{"name":"Adrian Escutia","title":"La Rebelion Founder","url":"https://adrian.escutia.me","page":{"permalink":"/authors/adrian"},"socials":{"x":"https://x.com/ades_mx","linkedin":"https://www.linkedin.com/in/adrianescutia/","github":"https://github.com/adrianescutia","newsletter":"https://rebelion.la"},"bio":"Adrian is the founder of La Rebelion, a newsletter about AI, technology, and the future. He is also a software engineer and entrepreneur.","imageURL":"https://github.com/adrianescutia.png","key":"adrian"}],"frontMatter":{"title":"Swagger/OAS v4 Is out","description":"Exploring the potential of OpenAPI Specification (OAS) v4 for AI tool integration and how it can complement Model Context Protocol (MCP).","authors":["adrian"],"tags":["oas","mcp","api","ai"],"image":"/img/ai-friend-chatting.png"},"unlisted":false,"prevItem":{"title":"MCP Threats and Misleadings - Prompt Injection","permalink":"/mcp-threats-prompt-injection"}},"content":"import ReactPlayer from \'react-player\'\\n\\n\ud83d\udea8 \\"OpenAPI Specification (OAS) v4 is out\\" - *That I wish*, this is the kind of headline I would expect to see soon, because OAS can easily be extended to enable RESTful APIs work seamlessly with AI. \\n\\nBy the end of this article, you\'ll know how to let any LLM call your REST tools automatically using OAS.\\n\\n\x3c!-- truncate --\x3e\\n\\n**What Does It Mean for AI Tool Integration?** Let\'s explore how OAS v4 could fit into the AI landscape, complementing protocols like Model Context Protocol (MCP). \\n\\n:::note  \\nThis is a speculative article about what I would like to see in OAS v4, based on my experience with Model Context Protocol (MCP) and AI tool integration. OAS v4 is not yet released, and this article is not endorsed by the OpenAPI Initiative or any other organization.  \\n:::\\n\\nOver the past decade, OAS has become the standard for describing RESTful APIs and is widely adopted across various industries. Now, it\'s time to expand it to cover AI use cases, addressing the evolving needs of developers.\\n\\nModel Context Protocol (MCP) has recently gained a lot of attention. It\'s great for local integrations, allows you to adjust the MCP server for backend connections, and with the latest updates, it can now handle remote calls, moving closer to RESTful APIs. \\n\\nSince the beginning [MCP was designed for local integrations mainly](https://rebelion.la/model-context-protocol-mcp-is-it-a-protocol-or-a-contract#heading-the-lsp-connection-understanding-mcps-roots). The breakthrough of MCP lies in its **ability to let the LLM discover the tools available**. This tool discovery feature marks MCP\'s unique value proposition, transforming the way we approach AI integrations. From there, everything falls into place similar to RESTful APIs: the client acts as a lightweight orchestration layer, the server handles the backend tasks, and the LLM plays the role of the brain, deciding which tools to use and how to use them.\\n\\n## How does MCP work? For mere mortals\\n\\nI have noticed that many people struggle to understand how MCP works, so let me try to explain it in a simple way.\\n\\nLet\'s imagine a conversation between the client (you), the LLM (a friend), and the server (weather guy):\\n\\nSetting the scene: you have a person (the MCP Server) who speaks a language you don\'t understand, such as French, and you have a friend who speaks both languages (The LLM), French and English. You\'d like to know the weather in Paris, so you can ask your friend for help. Your friend asks the person in French, gets the answer, and translates it back to you in English. In this scenario, your friend represents the LLM supporting tool calling natively. \\n\\nNow, imagine you can write in French but don\'t speak it fluently. You\'d like to know the weather in Paris, but this time you need someone to guide you on what to ask in French. You write the message asking for the weather. Once you receive the answer, you can translate it without help. This scenario reflects the role of an LLM that doesn\'t support tool calling natively, and the client has to call the tool directly.\\n\\nComplex, right? Let\'s see it in a sequence diagram:\\n\\n```mermaid\\nsequenceDiagram\\n  participant C as Client\\n  participant L as LLM\\n  participant S as Server\\n  C->>S: Fetch API spec (tools/list)\\n  S--\x3e>C: API spec (tools [list])\\n  C->>L: API spec (tools [list])\\n  C->>L: User input\\n  alt LLM supports tool calling\\n    L->>S: API call\\n    S--\x3e>L: API response\\n    L--\x3e>C: Result to user\\n  else LLM does not support tool calling\\n    L->>C: Tool selection/instruction\\n    C->>S: API call\\n    S--\x3e>C: API response\\n    C--\x3e>L: API response\\n    L--\x3e>C: Result to user\\n  end\\n```\\n\\nThe LLM receives a prompt with a description of the [available tools](https://modelcontextprotocol.io/specification/2025-06-18/server/tools#listing-tools). This tool list can be [mapped from the OpenAPI spec](https://rebelion.la/you-dont-need-to-implement-mcp-servers-a-contract-first-approach-to-ai-tool-integration?showSharer=true#heading-example-conversion-openapi-mcp). The LLM then chooses which tool to use and how to use it. The client simply passes the user input to the LLM, which decides what to do next. Some LLMs, like GPT-4-turbo and Llama2, support tool calling natively. Others, like Claude, do not, so the client calls the tool directly, based on the LLM\'s instruction.\\n\\nAs you can see, either the LLM supports tool calling natively or not, the flow is similar. The only difference is that in the latter case, the client does the call to the server, and forwards the response to the LLM to get the final result for the end-user - based on the LLM\'s instruction.\\n\\n## OAS v4 - The missing piece\\n\\nThe missing piece in OAS compared to MCP is that OAS does not have a discovery mechanism for the LLM to know which tools are available. This is where OAS v4 could shine, by adding a way for the LLM to discover the API spec and the available tools.\\n\\nImagine if OAS v4 had a way to describe the tools available in a way that the LLM could understand, and then the LLM could decide which tool to use, and how to use it. This would make it possible for any LLM to work seamlessly with RESTful APIs, without the need for a separate protocol like MCP, or even better, MCP could be used as a contract-first approach to AI tool integration, where the OAS v4 spec is the contract that defines the tools available, and the MCP server implements the backend logic. That\'s precisely what I have been advocating for a while now with my [contract-first approach to AI tool integration](https://rebelion.la/you-dont-need-to-implement-mcp-servers-a-contract-first-approach-to-ai-tool-integration?showSharer=true) and the *Headless API* (HAPI) initiative for MCP.\\n\\nNow that I have explained how MCP works, and how OAS v4 could be the missing piece, I hope you can see the potential of OAS v4 for AI tool integration. But, don\'t stop here, next is what I would like to see in OAS v4.\\n\\n## What You Should Expect in OAS v4\\n\\nOAS v4 is not just an incremental update\u2014it\'s an opportunity to rethink how APIs and AI tools work together. Here is what you should expect and advocate for in the next version, based on best practices and the needs of modern AI integrations.\\n\\n## Modular, Multi-File API Specifications\\n\\nYou need modular specs for real-world APIs. OAS v4 should support multi-file specifications natively, allowing you to break down large APIs into smaller, reusable modules (for example, pets, users, orders). This approach enhances maintainability and collaboration across teams. Instead of a single massive file, you organize your API into logical domains:\\n\\n- **API Root:** Main file with info, servers, and tags.\\n- **Security:** Dedicated files for OAuth2 and other schemes.\\n- **Paths by Domain:** Split endpoints into logical groups (e.g. `pet.api.yaml`, `user.api.yaml`).\\n- **Components/Models:** Reusable schemas separated by domain.\\n\\nThis modular structure keeps specs manageable (500\u20131000 lines each) and enables code generators and clients to see one unified spec. Tools like [Redocly CLI](https://redocly.com/docs/cli/file-management#one-large-file-to-many-small-ones) already offer splitting, but OAS v4 should standardize it. Modular specs also help AI tools (MCP servers) consume only the relevant modules, **improving performance and reducing cognitive load for LLMs** that may have context length limitations and make the LLMs\' job easier when deciding which tools to use.\\n\\n:::info\\nThe current `$ref` approach only allows referencing individual components, not entire files. Modular specs let you load what you need, when you need it.\\n:::\\n\\n**Benefits:**\\n- Parallel development and reviews\\n- Domain-specific clients (AI agents load only relevant modules)\\n- Easier maintenance and updates\\n- Improved collaboration\\n- Better tooling support\\n- Granular security schemes\\n\\n## Well-Known Tools Manifest for AI Discovery\\n\\nAI clients must discover available tools easily. OAS v4 should adopt a well-known URI convention (such as `/.well-known/mcp/tools-manifest.json`) so MCP clients and AI agents can auto-discover your API\'s tools. This follows patterns from [OpenID Connect Discovery](https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderConfig) and [OAuth 2.0 Security Scheme](https://spec.openapis.org/oas/v3.2.0.html#security-scheme-object). Give agents a predictable URL to learn your API\u2014this small change pays off big in discoverability.\\n\\n## AI-First Annotations and Metadata\\n\\nOAS v4 should introduce AI-first annotations to help clients and LLMs use APIs effectively. Add metadata extensions (such as `x-llm-hint`, `x-llm-example`) to endpoints, parameters, and responses. Use the [`kind`](https://spec.openapis.org/registry/tag-kind/) property to categorize tags for AI relevance. For example:\\n\\n```yaml\\ntags:\\n  - name: contacts\\n    summary: Manage contacts\\n    description: Endpoints for creating, updating, and searching contacts\\n    kind: ai-tool\\n\\n  - name: internal\\n    summary: Internal admin endpoints\\n    description: Endpoints not intended for AI or public use\\n    kind: internal\\n```\\n\\nWith this approach, AI agents and MCP servers filter and discover only the endpoints relevant for tool calling, ignoring internal or non-AI endpoints. You can further extend this pattern with custom extensions:\\n\\n```yaml\\npaths:\\n  /contacts/search:\\n    get:\\n      tags: [contacts]\\n      x-llm-hint: \\"Use this endpoint to search for contacts by name or email.\\"\\n      x-llm-example: \\"Find all contacts named Alice.\\"\\n```\\n\\nThese annotations guide LLMs on how to use specific endpoints, making your API more AI-friendly and discoverable.\\n\\n## References\\n\\n- [OpenAPI Specification](https://spec.openapis.org/oas/latest.html)\\n- [Redocly CLI: File Management](https://redocly.com/docs/cli/file-management#one-large-file-to-many-small-ones)\\n- [OpenID Connect Discovery](https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderConfig)\\n- [OWASP AI Security Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/)\\n\\nBy advocating for these features, you help shape OAS v4 into a standard that supports scalable, AI-ready APIs for the next generation of applications.\\n\\n## Example: Conversion OpenAPI \u2192 MCP\\n\\nHere is a simple example of how an OpenAPI spec can be converted to an MCP tools manifest.\\n\\nUsing the [Petstore example](https://petstore3.swagger.io/api/v3/openapi.json) from Swagger, we can extract the relevant information to create an MCP tools manifest. Deploying an MCP server with the [`hapi` server](https://docs.mcp.com.ai/components/hapi-server/), we can extend the OpenAPI spec with AI-first annotations and modular structure.\\n\\nIn the demo below, the Petstore API is extended to integrate WorkOS for authentication, and the OpenAPI spec is modularized into separate files for better organization.\\n\\n```yaml\\napiVersion: mcp.com.ai/v1\\nkind: Security\\nmetadata:\\n  name: hapi-security-config\\n  namespace: demo\\n  labels:\\n    app: demo\\n  annotations:\\n    description: HAPI server security configuration\\n    owner: team-hapi\\n    environment: development\\n    version: v1.0.0\\nspec:\\n  cors:\\n    enabled: true\\n    origin:\\n      - http://localhost:8080 # Your MCP Client\\n      - http://localhost:6274 # MCP Inspector\\n    headers:\\n      - Content-Type\\n      - Authorization\\n      - X-Requested-With\\n    methods:\\n      - GET\\n      - POST\\n      - PUT\\n      - DELETE\\n      - OPTIONS\\n  security:\\n    demo_auth:\\n      client_id: client_0\\n      well-known: https://*****.app/.well-known/oauth-authorization-server\\n  securitySchemes:\\n    demo_auth:\\n      type: oauth2\\n      flows:\\n        authorizationCode:\\n          authorizationUrl: https://api.workos.com/sso/authorize?connection=conn_01\\n          tokenUrl: https://*****.app/oauth2/token\\n          scopes:\\n            read: Read\\n            write: Modify\\n            admin: Access to admin operations\\n            read_all: Read private resources\\n```\\n\\n<ReactPlayer\\n  src=\'https://youtu.be/S2_Z0rbnOH8\'\\n  style={{ width: \'90%\', height: \'auto\', aspectRatio: \'4/3\' }}\\n  controls\\n/>\\n\\n## Conclusion\\n\\nOAS v4 has the potential to revolutionize how AI systems interact with RESTful APIs. By incorporating modular specs, AI-first annotations, and well-known discovery endpoints, OAS v4 can make it easier for LLMs to understand and utilize APIs effectively. This would not only benefit developers but also pave the way for more seamless AI integrations across various applications."}]}}')}}]);